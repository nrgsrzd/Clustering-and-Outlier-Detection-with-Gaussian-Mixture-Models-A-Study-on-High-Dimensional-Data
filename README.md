# Clustering-and-Outlier-Detection-with-Gaussian-Mixture-Models-A-Study-on-High-Dimensional-Data

In this project, the primary goal is to investigate the concepts of clustering and dimensionality reduction, which are fundamental in data analysis and machine learning.
GMM is a probabilistic model that is widely used for clustering tasks, and the project focuses on its application in detecting outliers in high-dimensional datasets. For this purpose, the MNIST dataset is selected as the target dataset. MNIST includes images of handwritten digits, making it suitable for testing. Certain digit classes, such as zero and six, are used as training data, while the digit eight is identified as an outlier. To deal with the high dimensionality of the data in the MNIST dataset, appropriate dimensionality reduction techniques are applied. The aim of these techniques is to reduce the computational complexity while preserving the essential properties of the data. Once the dataset is pre-processed and dimensionally reduced, the next step involves applying the GMM algorithm to detect outliers. GMM helps identify data points that deviate significantly from established patterns, thus removing outliers from the data set. Outliers are then removed to obtain a cleaner version of the dataset for further analysis. After removing the outliers, two classification algorithms, namely random forest and logistic regression, are used on both the original data set (with outliers) and the cleaned data set (without outliers). These algorithms use performance measures such as precision and recall are evaluated to evaluate their effectiveness in different scenarios. Hierarchical clustering is another clustering technique that is used to group similar data points into clusters. Gaussians of the GMM algorithm are placed and we compare the results. Before dealing with the project implementation details, we define the main concepts in this project. 1. Clustering of Gaussian mixture models (GMM): Gaussian mixture models (GMM) are a probabilistic model. is used to cluster data. It assumes that the data is generated from a mixture of several Gaussian distributions, each representing a cluster. The goal of GMM is to identify the underlying probability distribution of the data and assign each data point to a probability In the field of outlier detection, GMM can be used to identify data points that deviate significantly from the established clusters, so we mark them as outliers.
GMM clustering involves estimating parameters such as the mean, covariance, and weight of each Gaussian component to accurately model the data distribution and is usually used in applications where the data do not have clear cluster boundaries or when the clusters have different shapes and sizes. 2. Dimensionality reduction (PCA): The goal of dimensionality reduction techniques is to reduce the number of features (dimensions) in a data set while preserving its essential information.
PCA works by transforming the original high-dimensional data into a lower-dimensional space, where each dimension (principal component) captures the maximum variance of the data. By retaining only the principal components that contribute the most to the variance, PCA reduces computational complexity and noise in 3. Random Forest: Random forest is a popular ensemble learning method used for classification and regression tasks. It consists of a set of decision trees, where each tree is based on a random subset of features and data points are trained. Random forest combines the predictions of multiple decision trees for more accurate and robust predictions. 4. Logistic regression: Logistic regression is a statistical model used for binary classification tasks. The probability that a data input belongs modeled to a specific class using the logistic function (sigmoid function). Logistic regression estimates the parameters (coefficients) of the logistic function using optimization techniques such as gradient descent. 5. Hierarchical clustering: clustering Hierarchy is a clustering algorithm that organizes data points into a hierarchy of clusters. It does not require a predefined number of clusters and can be aggregated (from bottom to top) or divisive (from top to bottom). They are merged iteratively. Hierarchical clustering produces a dendrogram that visually shows the hierarchical structure of the clusters. Next, we discuss the implementation details.

# The outlier data in the dataset is as follows
<img width="438" alt="Screenshot 1402-12-24 at 5 52 01 in the afternoon" src="https://github.com/nrgsrzd/Clustering-and-Outlier-Detection-with-Gaussian-Mixture-Models-A-Study-on-High-Dimensional-Data/assets/66438749/4686af05-d576-4df9-b975-3b625112b04a">
# Result
<img width="555" alt="Screenshot 1402-12-24 at 5 52 35 in the afternoon" src="https://github.com/nrgsrzd/Clustering-and-Outlier-Detection-with-Gaussian-Mixture-Models-A-Study-on-High-Dimensional-Data/assets/66438749/1a59a861-b157-44f4-8dbf-8a8da2a3427d">
